{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "a9c5fc42-91ab-4b3f-ac0a-056d6095b51a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygit2==1.15.1 in /usr/local/lib/python3.10/dist-packages (1.15.1)\n",
            "Requirement already satisfied: cffi>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pygit2==1.15.1) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.16.0->pygit2==1.15.1) (2.22)\n",
            "/content\n",
            "fatal: destination path 'Fooocus' already exists and is not an empty directory.\n",
            "/content/Fooocus\n",
            "Already up-to-date\n",
            "Update succeeded.\n",
            "[System ARGV] ['entry_with_update.py', '--share', '--always-high-vram']\n",
            "Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
            "Fooocus version: 2.5.5\n",
            "[Cleanup] Attempting to delete content of temp dir /tmp/fooocus\n",
            "[Cleanup] Cleanup successful\n",
            "Total VRAM 15102 MB, total RAM 12979 MB\n",
            "Set vram state to: HIGH_VRAM\n",
            "Always offload VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype: torch.float32\n",
            "Using pytorch cross attention\n",
            "Refiner unloaded.\n",
            "IMPORTANT: You are using gradio version 3.41.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on local URL:  http://127.0.0.1:7865\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Running on public URL: https://8e01f9fe36646d84fe.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "extra {'cond_stage_model.clip_l.text_projection', 'cond_stage_model.clip_l.logit_scale'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [('sd_xl_offset_example-lora_1.0.safetensors', 0.1)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 0.1.\n",
            "Fooocus V2 Expansion: Vocab with 642 words.\n",
            "Fooocus Expansion engine loaded for cuda:0, use_fp16 = True.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.60 seconds\n",
            "2024-10-31 12:13:02.760421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-31 12:13:03.096896: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-31 12:13:03.193805: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-31 12:13:03.724603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-31 12:13:05.898386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Started worker with PID 3111\n",
            "App started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865 or https://8e01f9fe36646d84fe.gradio.live\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 1780194563883711693\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 60 - 30\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1160, in handler\n",
            "    tasks, use_expansion, loras, current_progress = process_prompt(async_task, async_task.prompt, async_task.negative_prompt,\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 661, in process_prompt\n",
            "    pipeline.refresh_everything(refiner_model_name=async_task.refiner_model_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 250, in refresh_everything\n",
            "    refresh_base_model(base_model_name, vae_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 74, in refresh_base_model\n",
            "    model_base = core.load_model(filename, vae_filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/core.py\", line 147, in load_model\n",
            "    unet, clip, vae, vae_filename, clip_vision = load_checkpoint_guess_config(ckpt_filename, embedding_directory=path_embeddings,\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sd.py\", line 449, in load_checkpoint_guess_config\n",
            "    model_config = model_detection.model_config_from_unet(sd, \"model.diffusion_model.\", unet_dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 163, in model_config_from_unet\n",
            "    unet_config = detect_unet_config(state_dict, unet_key_prefix, dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 49, in detect_unet_config\n",
            "    model_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[0]\n",
            "KeyError: 'model.diffusion_model.input_blocks.0.0.weight'\n",
            "Total time: 0.14 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 7721986132178370308\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 60 - 30\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1160, in handler\n",
            "    tasks, use_expansion, loras, current_progress = process_prompt(async_task, async_task.prompt, async_task.negative_prompt,\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 661, in process_prompt\n",
            "    pipeline.refresh_everything(refiner_model_name=async_task.refiner_model_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 250, in refresh_everything\n",
            "    refresh_base_model(base_model_name, vae_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 74, in refresh_base_model\n",
            "    model_base = core.load_model(filename, vae_filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/core.py\", line 147, in load_model\n",
            "    unet, clip, vae, vae_filename, clip_vision = load_checkpoint_guess_config(ckpt_filename, embedding_directory=path_embeddings,\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sd.py\", line 449, in load_checkpoint_guess_config\n",
            "    model_config = model_detection.model_config_from_unet(sd, \"model.diffusion_model.\", unet_dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 163, in model_config_from_unet\n",
            "    unet_config = detect_unet_config(state_dict, unet_key_prefix, dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 49, in detect_unet_config\n",
            "    model_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[0]\n",
            "KeyError: 'model.diffusion_model.input_blocks.0.0.weight'\n",
            "Total time: 0.05 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 1363511552536637008\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 60 - 60\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1160, in handler\n",
            "    tasks, use_expansion, loras, current_progress = process_prompt(async_task, async_task.prompt, async_task.negative_prompt,\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 661, in process_prompt\n",
            "    pipeline.refresh_everything(refiner_model_name=async_task.refiner_model_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 249, in refresh_everything\n",
            "    refresh_refiner_model(refiner_model_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 96, in refresh_refiner_model\n",
            "    model_refiner = core.load_model(filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/core.py\", line 147, in load_model\n",
            "    unet, clip, vae, vae_filename, clip_vision = load_checkpoint_guess_config(ckpt_filename, embedding_directory=path_embeddings,\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sd.py\", line 449, in load_checkpoint_guess_config\n",
            "    model_config = model_detection.model_config_from_unet(sd, \"model.diffusion_model.\", unet_dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 163, in model_config_from_unet\n",
            "    unet_config = detect_unet_config(state_dict, unet_key_prefix, dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 49, in detect_unet_config\n",
            "    model_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[0]\n",
            "KeyError: 'model.diffusion_model.input_blocks.0.0.weight'\n",
            "Total time: 0.06 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 3448628889834206030\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 60 - 60\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1160, in handler\n",
            "    tasks, use_expansion, loras, current_progress = process_prompt(async_task, async_task.prompt, async_task.negative_prompt,\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 661, in process_prompt\n",
            "    pipeline.refresh_everything(refiner_model_name=async_task.refiner_model_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 249, in refresh_everything\n",
            "    refresh_refiner_model(refiner_model_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 96, in refresh_refiner_model\n",
            "    model_refiner = core.load_model(filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/core.py\", line 147, in load_model\n",
            "    unet, clip, vae, vae_filename, clip_vision = load_checkpoint_guess_config(ckpt_filename, embedding_directory=path_embeddings,\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sd.py\", line 449, in load_checkpoint_guess_config\n",
            "    model_config = model_detection.model_config_from_unet(sd, \"model.diffusion_model.\", unet_dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 163, in model_config_from_unet\n",
            "    unet_config = detect_unet_config(state_dict, unet_key_prefix, dtype)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_detection.py\", line 49, in detect_unet_config\n",
            "    model_channels = state_dict['{}input_blocks.0.0.weight'.format(key_prefix)].shape[0]\n",
            "KeyError: 'model.diffusion_model.input_blocks.0.0.weight'\n",
            "Total time: 0.06 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 3165472351470173600\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 60 - 60\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] Create an architectural visualization of a modern two-story house at sunset, featuring a striking cubic design with clean, minimalistic lines. The upper floor has three box-like sections extending outward, each containing large glass windows and sliding doors that open onto balconies framed with dark, rectangular structures. The walls inside the balconies are finished in natural wood, adding warmth to the modern look. On the ground floor, full-height glass doors provide a seamless indoor-outdoor connection. There is a cozy wooden deck with colorful modern outdoor chairs, and a warm glow from the interior lights spills out onto the surroundings. The sky is a soft lavender with hints of pink, and the foreground includes a mix of greenery and mature trees, emphasizing a harmonious blend of nature and contemporary architecture, detailed, sharp focus, excellent composition, cinematic atmosphere, dynamic dramatic beautiful light, aesthetic, very inspirational, innocent, intricate, highly detail, clear, color, inspired, rich vibrant colors, epic, stunning, gorgeous, amazing, inspiring, thought, perfect, cute, fantastic, awesome, incredible, fabulous, great, luxury, elegant\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.16 seconds\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (896, 1152)\n",
            "Preparation time: 4.72 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/1 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.90 seconds\n",
            "100% 60/60 [01:01<00:00,  1.03s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\n",
            "[Fooocus] Saving image 1/1 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2024-10-31/log.html\n",
            "Generating and saving time: 66.63 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 66.63 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 71.39 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.96 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip install pygit2==1.15.1\n",
        "%cd /content\n",
        "!git clone https://github.com/lllyasviel/Fooocus.git\n",
        "%cd /content/Fooocus\n",
        "!python entry_with_update.py --share --always-high-vram\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}